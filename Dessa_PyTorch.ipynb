{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dessa_PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "biJ3IeocYLJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "401b0c28-7ffb-4ae9-a1f1-275d293edcb0"
      },
      "source": [
        "!pip install soundfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP-25yr5Xy1D"
      },
      "source": [
        "import os\n",
        "import tqdm\n",
        "import torch\n",
        "import h5py\n",
        "import librosa\n",
        "import random\n",
        "from adamp import AdamP\n",
        "from utils import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from scipy import stats\n",
        "import soundfile as sf\n",
        "from scipy.signal import spectrogram\n",
        "from IPython.display import Audio\n",
        "import librosa.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6XXtcyzWuL8"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L1fMNIoWwOG"
      },
      "source": [
        "sample_file = '/data/Audio/LA/ASVspoof2019_LA_train/flac/LA_T_1000137.flac'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL69lbD9jHTA"
      },
      "source": [
        "# read the labels from the file: bonafide vs spoof\n",
        "# labels: 1 = true sample, 0 = spoofed sample\n",
        "filename = 'ASVspoof2019.LA.cm.train.trn.txt'\n",
        "dir_path = '/data/Audio/LA/ASVspoof2019_LA_cm_protocols'\n",
        "train_label_dict = {}\n",
        "with open(os.path.join(dir_path, filename)) as f:\n",
        "    for line in f:\n",
        "        toks = line.split()\n",
        "        sample = toks[1]\n",
        "        label = 1 if toks[-1] == 'bonafide' else 0\n",
        "        train_label_dict[sample] = label\n",
        "print(f'There are {len(train_label_dict)} entries')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ2RVq_KjIEg"
      },
      "source": [
        "# labels: 1 = true sample, 0 = spoofed sample\n",
        "filename = 'ASVspoof2019.LA.cm.eval.trl.txt'\n",
        "dir_path = '/data/Audio/LA/ASVspoof2019_LA_cm_protocols'\n",
        "eval_label_dict = {}\n",
        "with open(os.path.join(dir_path, filename)) as f:\n",
        "    for line in f:\n",
        "        toks = line.split()\n",
        "        sample = toks[1]\n",
        "        label = 1 if toks[-1] == 'bonafide' else 0\n",
        "        eval_label_dict[sample] = label\n",
        "print(f'There are {len(eval_label_dict)} entries')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GTXkSfojKHm"
      },
      "source": [
        "def pre_emp(x):\n",
        "    '''\n",
        "    Apply pre-emphasis to given utterance.\n",
        "    x: list or 1 dimensional numpy.ndarray\n",
        "    '''\n",
        "    return np.asarray(x[1:] - 0.97 * x[:-1], dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Yk7JKYjMa3"
      },
      "source": [
        "def preprocess_and_save_audio(dirpath, label_dict, recompute=False, max_files=None):\n",
        "    '''\n",
        "    Load the audio flac files, trim the audio file (decibel level) and\n",
        "    compute the Mel spectrogram for the audio.\n",
        "    '''\n",
        "    filenames = os.listdir(os.path.join(dirpath, 'flac'))\n",
        "    if not os.path.isfile(os.path.join(dirpath, 'preproc', 'preproc.npy')) or recompute:\n",
        "        precproc_list = []\n",
        "        num_removed = 0\n",
        "        num_processed = 0\n",
        "        for filename in tqdm.notebook.tqdm(filenames):\n",
        "            audio_array, sample_rate = librosa.load(os.path.join(dirpath, 'flac', filename), sr=16000)\n",
        "            trim_audio_array, index = librosa.effects.trim(audio_array)\n",
        "            mel_spec_array = librosa.feature.melspectrogram(y=trim_audio_array, sr=sample_rate,\n",
        "                                                            n_mels=num_mels).T\n",
        "            label_name = filename.split('.')[0]\n",
        "            try:\n",
        "                label = label_dict[label_name]\n",
        "                precproc_list.append((mel_spec_array, label))\n",
        "                num_processed += 1\n",
        "            except KeyError:\n",
        "                num_removed += 1\n",
        "                print(f'Removed: {num_removed} removed', end='\\r')\n",
        "                os.remove(os.path.join(dirpath, 'flac', filename))\n",
        "            # limit \n",
        "            if max_files != None and num_processed > max_files:\n",
        "                break\n",
        "        if not os.path.isdir(os.path.join(dirpath, 'preproc')):\n",
        "            os.mkdir(os.path.join(dirpath, 'preproc'))\n",
        "        np.save(os.path.join(dirpath, 'preproc', 'preproc.npy'), precproc_list)\n",
        "    else:\n",
        "        print(\"Preprocessing already done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CXhtER2jVAA"
      },
      "source": [
        "class AudioDS(Dataset):\n",
        "    def __init__(self, x_arr, y_arr, stime, train=True):\n",
        "        # record indices of the spoofs & bonafide samples\n",
        "        # Note: in training, spoofs vastly outnumber bonafides\n",
        "        y_spoof_indices = [i for i, v in enumerate(y_arr) if v == 0]\n",
        "        y_bonafide_indices = [i for i, v in enumerate(y_arr) if v == 1]\n",
        "        assert(len(y_bonafide_indices) < len(y_spoof_indices))\n",
        "        \n",
        "        # create the indices that will comprise this DS\n",
        "        random.shuffle(y_spoof_indices)\n",
        "        random.shuffle(y_bonafide_indices)\n",
        "        \n",
        "        # ds indices\n",
        "        if train:\n",
        "            num_each = min(len(y_bonafide_indices),len(y_spoof_indices))\n",
        "            ds_indices = y_spoof_indices[:num_each] + y_bonafide_indices[:num_each]\n",
        "        else:\n",
        "            ds_indices = y_spoof_indices + y_bonafide_indices\n",
        "            \n",
        "        random.shuffle(ds_indices)\n",
        "        \n",
        "        # collect the samples\n",
        "        x = [x_arr[i] for i in ds_indices]\n",
        "        y = [y_arr[i] for i in ds_indices]\n",
        "        \n",
        "        self.x = x.copy()\n",
        "        self.y = y.copy()\n",
        "        self.stime = stime\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        X = self.x[idx]\n",
        "        y = self.y[idx]\n",
        "        stime = X.shape[0]\n",
        "        # sample is of shorter duration than minimum\n",
        "        if stime == self.stime:\n",
        "            X = X.reshape(1, stime, num_mels)\n",
        "        elif stime < self.stime:\n",
        "            x_dup = int(self.stime / stime) + 1\n",
        "            X = np.tile(X, (1, x_dup, 1))[:, :self.stime, :]\n",
        "        else:\n",
        "            start_idx = np.random.randint(low = 0, high = stime - self.stime)\n",
        "            X = X.reshape(1, stime, num_mels)\n",
        "            X = X[:, start_idx:start_idx+self.stime, :]\n",
        "        # return the sample\n",
        "        x = torch.from_numpy(X)\n",
        "        y = torch.tensor(y)\n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dMu41PoYuCK"
      },
      "source": [
        "class Dessa(torch.nn.module):\n",
        "\n",
        "  def __init__(self, batch_size, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.linear1 = torch.nn.Linear(input_size, out_features=80)\n",
        "    self.conv1d_19 = torch.nn.Conv1D(80, 16)\n",
        "    self.conv1d_20 = torch.nn.Conv1D(80, 16)\n",
        "    self.conv1d_21 = torch.nn.Conv1D(80, 16)\n",
        "    self.leaky_re_lu_20 = torch.nn.LeakyReLU()\n",
        "    self.leaky_re_lu_21 = torch.nn.LeakyReLU()\n",
        "    self.leaky_re_lu_22 = torch.nn.LeakyReLU()\n",
        "    self.spatial_dropout1d_6 = torch.nn.Dropout2d() #Slight issue with dropout mask used as compared to spatial droput - worth looking into later with Vijay.\n",
        "    self.spatial_dropout1d_7 = torch.nn.Dropout2d()\n",
        "    self.spatial_dropout1d_8 = torch.nn.Dropout2d()\n",
        "    self.conv1d_22 = torch.nnConv1D(16, 32)\n",
        "    self.conv1d_23 = torch.nnConv1D(16, 32)\n",
        "    self.conv1d_24 = torch.nnConv1D(16, 32)\n",
        "    self.leaky_re_lu_23 = torch.nn.LeakyReLU()\n",
        "    self.leaky_re_lu_24 = torch.nn.LeakyReLU()\n",
        "    self.leaky_re_lu_25 = torch.nn.LeakyReLU()\n",
        "    self.lambda_4 = torch.nn.Identity()\n",
        "    self.lambda_5 = torch.nn.Identity()\n",
        "    self.lambda_6 = torch.nn.Identity()\n",
        "\n",
        "    #Write own concatenate layer\n",
        "    self.concatenate_1 = self.concatenate_layer()\n",
        "\n",
        "    self.dense_3 = torch.nn.Linear(96, 10)\n",
        "    self.batch_norm_2 = torch.nn.BatchNorm1d()\n",
        "    self.leaky_re_lu_26 = torch.nn.LeakyReLU()\n",
        "    self.dropout_1 = torch.nn.Dropout()\n",
        "    self.dense_4 = torch.nn.Linear(10, 10)\n",
        "    self.batch_norm_3 = torch.nn.BatchNorm1d()\n",
        "    self.leaky_re_lu_27 = torch.nn.LeakyReLU()\n",
        "    self.dropout_2 = torch.nn.Dropout()\n",
        "    self.dense_5 = torch.nn.Linear(10, 10)\n",
        "    self.activation = torch.nn.Sigmoid()\n",
        "\n",
        "  def concatenate_layer(self):\n",
        "    #HOF\n",
        "    def concatenate(tensor_array):\n",
        "      return torch.cat([tensor_array], dim=0)\n",
        "    return concatenate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D5NpKgwjftA"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    ds = AudioDS(xtrain, ytrain, stime=sample_ts)\n",
        "    dl = DataLoader(ds, batch_size=8, shuffle=True)\n",
        "    \n",
        "    epoch_loss = []\n",
        "    \n",
        "    net.train()\n",
        "    \n",
        "    with torch.set_grad_enabled(True):\n",
        "        for m_batch, m_label in tqdm.notebook.tqdm(dl):\n",
        "            m_batch, m_label = m_batch.to(device), m_label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            code, output = net(m_batch)\n",
        "            \n",
        "            loss = criterion(output, m_label)\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        eloss = np.array(epoch_loss)\n",
        "        print(f'Epoch[{epoch}]: epoch train loss: {np.mean(eloss):.3f}')\n",
        "        del dl\n",
        "        del ds\n",
        "    \n",
        "    scheduler.step(np.mean(eloss))\n",
        "    \n",
        "    net.eval()\n",
        "    val_loss = []\n",
        "    \n",
        "    with torch.set_grad_enabled(False):\n",
        "        # create dataset, dataloader - randomized, but balanced\n",
        "        ds = AudioDS(xeval, yeval, stime=sample_ts, train=False)\n",
        "        dl = DataLoader(ds, batch_size=8, shuffle=True)\n",
        "    \n",
        "        total_samples = 0\n",
        "        total_corrects = 0\n",
        "        epoch_loss = []\n",
        "\n",
        "        for m_batch, m_label in tqdm.notebook.tqdm(dl):\n",
        "            m_batch, m_label = m_batch.to(device), m_label.to(device)\n",
        "\n",
        "            _, output = net(m_batch)\n",
        "            \n",
        "            loss = criterion(output, m_label)\n",
        "            \n",
        "            val_loss.append(loss.item())\n",
        "            \n",
        "            _, preds = torch.max(output, 1)\n",
        "            output = output.detach().cpu()\n",
        "            \n",
        "            total_corrects += torch.sum(preds.cpu() == m_label.cpu()).item()\n",
        "            total_samples += len(m_label)\n",
        "        \n",
        "        acc = float(total_corrects*100.0/total_samples)\n",
        "        print(f'Epoch[{epoch}]: epoch accuracy: {acc:.2f}')\n",
        "        \n",
        "        eloss = np.array(val_loss)\n",
        "        print(f'Epoch[{epoch}]: epoch validation loss: {np.mean(eloss):.3f}')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}